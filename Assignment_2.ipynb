{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1**\n"
      ],
      "metadata": {
        "id": "28e43n6pPjrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pretrained Word2Vec Google News model\n",
        "wv_pretrained = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# --- Task 1: Find similar words ---\n",
        "words = ['king', 'apple', 'computer', 'music', 'university']\n",
        "\n",
        "for word in words:\n",
        "    print(f\"\\nTop 5 words similar to '{word}':\")\n",
        "    for sim_word, score in wv_pretrained.most_similar(word, topn=5):\n",
        "        print(f\"{sim_word}: {score:.4f}\")\n",
        "\n",
        "# --- Task 2: Word Vector Arithmetic (Analogies) ---\n",
        "print(\"\\n=== Vector Arithmetic Analogies ===\")\n",
        "analogies = [\n",
        "    (\"king\", \"man\", \"woman\"),     # Expected: queen\n",
        "    (\"Paris\", \"France\", \"Germany\"),  # Expected: Berlin\n",
        "    (\"walking\", \"walk\", \"swim\")   # Expected: swimming\n",
        "]\n",
        "\n",
        "for a, b, c in analogies:\n",
        "    result = wv_pretrained.most_similar(positive=[c, a], negative=[b], topn=1)\n",
        "    print(f\"{a} - {b} + {c} ≈ {result[0][0]} (Score: {result[0][1]:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RYGITu-P6Sp",
        "outputId": "a613d191-9775-4ea9-f1a7-eecea182fac8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "\n",
            "Top 5 words similar to 'king':\n",
            "kings: 0.7138\n",
            "queen: 0.6511\n",
            "monarch: 0.6413\n",
            "crown_prince: 0.6204\n",
            "prince: 0.6160\n",
            "\n",
            "Top 5 words similar to 'apple':\n",
            "apples: 0.7204\n",
            "pear: 0.6451\n",
            "fruit: 0.6410\n",
            "berry: 0.6302\n",
            "pears: 0.6134\n",
            "\n",
            "Top 5 words similar to 'computer':\n",
            "computers: 0.7979\n",
            "laptop: 0.6640\n",
            "laptop_computer: 0.6549\n",
            "Computer: 0.6473\n",
            "com_puter: 0.6082\n",
            "\n",
            "Top 5 words similar to 'music':\n",
            "classical_music: 0.7198\n",
            "jazz: 0.6835\n",
            "Music: 0.6596\n",
            "Without_Donny_Kirshner: 0.6416\n",
            "songs: 0.6396\n",
            "\n",
            "Top 5 words similar to 'university':\n",
            "universities: 0.7004\n",
            "faculty: 0.6781\n",
            "unversity: 0.6758\n",
            "undergraduate: 0.6587\n",
            "univeristy: 0.6585\n",
            "\n",
            "=== Vector Arithmetic Analogies ===\n",
            "king - man + woman ≈ queen (Score: 0.7118)\n",
            "Paris - France + Germany ≈ Berlin (Score: 0.7644)\n",
            "walking - walk + swim ≈ swimming (Score: 0.8246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2**"
      ],
      "metadata": {
        "id": "ywIAeyv1Q4dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn nltk gensim tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEOELCJFS3Ju",
        "outputId": "4dd897d4-c798-429d-d185-4590a2e63d11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.downloader import load\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# --- Load dataset safely ---\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\", quotechar='\"', escapechar='\\\\')\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# --- Clean + tokenize ---\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = text.lower().split()\n",
        "    return [w for w in tokens if w not in stop_words]\n",
        "\n",
        "df['tokens'] = df['review'].apply(preprocess)\n",
        "\n",
        "# --- Train/Test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Vector Averaging ---\n",
        "def get_vectors(tokens_list, model, vector_size):\n",
        "    vectors = []\n",
        "    for tokens in tokens_list:\n",
        "        word_vecs = [model[word] for word in tokens if word in model]\n",
        "        if word_vecs:\n",
        "            vectors.append(np.mean(word_vecs, axis=0))\n",
        "        else:\n",
        "            vectors.append(np.zeros(vector_size))\n",
        "    return np.array(vectors)\n",
        "\n",
        "# --- 1. Pretrained Word2Vec ---\n",
        "print(\"Using Pretrained Word2Vec...\")\n",
        "pre_w2v = load(\"word2vec-google-news-300\")\n",
        "X_train_wv = get_vectors(X_train, pre_w2v, 300)\n",
        "X_test_wv = get_vectors(X_test, pre_w2v, 300)\n",
        "clf_wv = LogisticRegression(max_iter=1000)\n",
        "clf_wv.fit(X_train_wv, y_train)\n",
        "acc_wv = accuracy_score(y_test, clf_wv.predict(X_test_wv))\n",
        "\n",
        "# --- 2. Custom Skip-gram Word2Vec ---\n",
        "print(\"Training custom Skip-gram Word2Vec...\")\n",
        "custom_sg = Word2Vec(sentences=X_train.tolist(), sg=1, vector_size=100, window=3, min_count=1, workers=4).wv\n",
        "X_train_sg = get_vectors(X_train, custom_sg, 100)\n",
        "X_test_sg = get_vectors(X_test, custom_sg, 100)\n",
        "clf_sg = LogisticRegression(max_iter=1000)\n",
        "clf_sg.fit(X_train_sg, y_train)\n",
        "acc_sg = accuracy_score(y_test, clf_sg.predict(X_test_sg))\n",
        "\n",
        "# --- 3. Custom CBOW Word2Vec ---\n",
        "print(\"Training custom CBOW Word2Vec...\")\n",
        "custom_cb = Word2Vec(sentences=X_train.tolist(), sg=0, vector_size=100, window=3, min_count=1, workers=4).wv\n",
        "X_train_cb = get_vectors(X_train, custom_cb, 100)\n",
        "X_test_cb = get_vectors(X_test, custom_cb, 100)\n",
        "clf_cb = LogisticRegression(max_iter=1000)\n",
        "clf_cb.fit(X_train_cb, y_train)\n",
        "acc_cb = accuracy_score(y_test, clf_cb.predict(X_test_cb))\n",
        "\n",
        "# --- 4. Custom FastText ---\n",
        "print(\"Training custom FastText...\")\n",
        "custom_ft = FastText(sentences=X_train.tolist(), sg=1, vector_size=100, window=3, min_count=1, workers=4).wv\n",
        "X_train_ft = get_vectors(X_train, custom_ft, 100)\n",
        "X_test_ft = get_vectors(X_test, custom_ft, 100)\n",
        "clf_ft = LogisticRegression(max_iter=1000)\n",
        "clf_ft.fit(X_train_ft, y_train)\n",
        "acc_ft = accuracy_score(y_test, clf_ft.predict(X_test_ft))\n",
        "\n",
        "# --- Results ---\n",
        "print(\"\\n=== Model Accuracy Summary ===\")\n",
        "print(f\"Pretrained Word2Vec  : {acc_wv:.4f}\")\n",
        "print(f\"Custom Skip-gram W2V : {acc_sg:.4f}\")\n",
        "print(f\"Custom CBOW W2V      : {acc_cb:.4f}\")\n",
        "print(f\"Custom FastText      : {acc_ft:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxsiPE9XSMdI",
        "outputId": "02b7543a-d216-4612-93d4-787d0d09c428"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Pretrained Word2Vec...\n",
            "Training custom Skip-gram Word2Vec...\n",
            "Training custom CBOW Word2Vec...\n",
            "Training custom FastText...\n",
            "\n",
            "=== Model Accuracy Summary ===\n",
            "Pretrained Word2Vec  : 0.8475\n",
            "Custom Skip-gram W2V : 0.8679\n",
            "Custom CBOW W2V      : 0.8514\n",
            "Custom FastText      : 0.8644\n"
          ]
        }
      ]
    }
  ]
}