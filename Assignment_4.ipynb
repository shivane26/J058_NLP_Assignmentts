{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1) upgrade pip tooling\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# 2) small system deps to reduce build errors (Colab only)\n",
        "!apt-get update -y -qq && apt-get install -y -qq build-essential\n",
        "\n",
        "# 3) minimal python packages â€” note: no `evaluate`\n",
        "!pip install -q transformers datasets scikit-learn accelerate pyarrow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWly9ireUE_Z",
        "outputId": "652d0728-f065-4163-ae67-a98ba42d3ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Minimal, reliable pipeline (no transformers) ===\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# --- Load dataset ---\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "# expected columns: 'review', 'sentiment'\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "df['label'] = df['sentiment'].str.strip().str.lower().map({'positive': 1, 'negative': 0})\n",
        "df = df[['review', 'label']].dropna().reset_index(drop=True)\n",
        "print(\"Total examples:\", len(df))\n",
        "\n",
        "# --- Train/val/test split (80/10/10) ---\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=SEED)\n",
        "print(len(train_df), len(val_df), len(test_df), \"-> train/val/test sizes\")\n",
        "\n",
        "# --- Create a reduced subset for model selection (stratified) ---\n",
        "subset_train_size = 8000\n",
        "subset_val_size = 2000\n",
        "\n",
        "def stratified_subset(df, n, seed=SEED):\n",
        "    pos = df[df.label==1]\n",
        "    neg = df[df.label==0]\n",
        "    n_pos = int(round(n * len(pos) / len(df)))\n",
        "    n_neg = n - n_pos\n",
        "    pos_samp = pos.sample(n=n_pos, random_state=seed)\n",
        "    neg_samp = neg.sample(n=n_neg, random_state=seed)\n",
        "    return pd.concat([pos_samp, neg_samp]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "subset_train = stratified_subset(train_df, subset_train_size)\n",
        "subset_val   = stratified_subset(val_df, subset_val_size)\n",
        "\n",
        "print(\"Subset sizes:\", len(subset_train), len(subset_val))\n",
        "\n",
        "# --- Models to compare (5 classical, fast options) ---\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=400, random_state=SEED),\n",
        "    \"LinearSVC\": LinearSVC(max_iter=4000, random_state=SEED),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=200, random_state=SEED),\n",
        "    \"MultinomialNB\": MultinomialNB()\n",
        "}\n",
        "\n",
        "# We use TF-IDF features (unigram+bigram), max_features limit for speed\n",
        "tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2), stop_words='english')\n",
        "\n",
        "# --- Helper: custom metric (binary F1) ---\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall, \"accuracy\": acc}\n",
        "\n",
        "# --- Fit TF-IDF on subset training text once and transform ---\n",
        "X_sub_train = tfidf.fit_transform(subset_train['review'].values)\n",
        "X_sub_val = tfidf.transform(subset_val['review'].values)\n",
        "y_sub_train = subset_train['label'].values\n",
        "y_sub_val   = subset_val['label'].values\n",
        "\n",
        "# --- Compare models on subset ---\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    print(\"Training (subset) ->\", name)\n",
        "    # For SVC which doesn't support predict_proba, we can wrap in CalibratedClassifierCV if you need probabilities.\n",
        "    clf = model\n",
        "    # Fit\n",
        "    try:\n",
        "        clf.fit(X_sub_train, y_sub_train)\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR training {name}: {e}\")\n",
        "        results.append((name, None, None))\n",
        "        continue\n",
        "    preds = clf.predict(X_sub_val)\n",
        "    metrics = compute_metrics(y_sub_val, preds)\n",
        "    print(f\"  {name} metrics: f1={metrics['f1']:.4f} prec={metrics['precision']:.4f} rec={metrics['recall']:.4f} acc={metrics['accuracy']:.4f}\")\n",
        "    results.append((name, metrics['f1'], clf))\n",
        "\n",
        "# --- Pick best by F1 (fallback to first if tie) ---\n",
        "results_sorted = sorted([r for r in results if r[1] is not None], key=lambda x: x[1], reverse=True)\n",
        "if not results_sorted:\n",
        "    raise RuntimeError(\"All models failed during subset training.\")\n",
        "best_name, best_f1, best_clf_on_subset = results_sorted[0]\n",
        "print(\"\\nBest on subset:\", best_name, \"with F1 =\", best_f1)\n",
        "\n",
        "# --- Now retrain best model on full train+val (the 'full training' stage) ---\n",
        "# Fit TF-IDF on full training text (train + validation combined) for best final model\n",
        "full_train = pd.concat([train_df, val_df]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "X_full_train = tfidf.fit_transform(full_train['review'].values)\n",
        "y_full_train = full_train['label'].values\n",
        "\n",
        "# Recreate the best classifier instance (fresh) from name to avoid carryover\n",
        "best_model_cls = models[best_name]\n",
        "print(\"\\nRetraining best model on full training set:\", best_name)\n",
        "best_model_final = best_model_cls\n",
        "best_model_final.fit(X_full_train, y_full_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "X_test = tfidf.transform(test_df['review'].values)\n",
        "y_test = test_df['label'].values\n",
        "y_pred_test = best_model_final.predict(X_test)\n",
        "final_metrics = compute_metrics(y_test, y_pred_test)\n",
        "print(\"\\nFinal test metrics for\", best_name, \":\", final_metrics)\n",
        "\n",
        "# If model supports probabilities, get them; else use decision function and map to pseudo-probability\n",
        "def get_probs(clf, X):\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        return clf.predict_proba(X)  # shape (n, classes)\n",
        "    elif hasattr(clf, \"decision_function\"):\n",
        "        # for binary, map decision_function to probabilities via logistic sigmoid\n",
        "        df = clf.decision_function(X)\n",
        "        # if multiclass, fallback to zeros\n",
        "        if df.ndim == 1:\n",
        "            from scipy.special import expit\n",
        "            p_pos = expit(df)\n",
        "            return np.vstack([1-p_pos, p_pos]).T\n",
        "        else:\n",
        "            # fallback: normalize softmax\n",
        "            from scipy.special import softmax\n",
        "            return softmax(df, axis=1)\n",
        "    else:\n",
        "        # fallback: deterministic one-hot from predict\n",
        "        preds = clf.predict(X)\n",
        "        probs = np.zeros((len(preds), 2))\n",
        "        probs[np.arange(len(preds)), preds] = 1.0\n",
        "        return probs\n",
        "\n",
        "# --- Inference on 10 random test reviews ---\n",
        "sampled = test_df.sample(10, random_state=SEED).reset_index(drop=True)\n",
        "X_sample = tfidf.transform(sampled['review'].values)\n",
        "probs = get_probs(best_model_final, X_sample)\n",
        "preds_sample = np.argmax(probs, axis=1)\n",
        "\n",
        "for i, row in sampled.iterrows():\n",
        "    txt = row['review']\n",
        "    true_label = \"positive\" if row['label'] == 1 else \"negative\"\n",
        "    pred_label = \"positive\" if preds_sample[i] == 1 else \"negative\"\n",
        "    p_pos = float(probs[i,1])\n",
        "    print(\"----- Example\", i+1, \"-----\")\n",
        "    print(\"Review (first 400 chars):\", txt[:400].replace(\"\\n\",\" \"))\n",
        "    print(f\"True: {true_label}  Pred: {pred_label}  Prob_pos: {p_pos:.3f}\")\n",
        "    print()\n",
        "\n",
        "# --- Save best model and vectorizer for later use ---\n",
        "import joblib\n",
        "joblib.dump(best_model_final, \"best_model.pkl\")\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
        "print(\"Saved best_model.pkl and tfidf_vectorizer.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sKEsRZoVi__",
        "outputId": "1eedc0b5-2f59-4c9e-eb38-27848c451d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['review', 'sentiment']\n",
            "Total examples: 50000\n",
            "40000 5000 5000 -> train/val/test sizes\n",
            "Subset sizes: 8000 2000\n",
            "Training (subset) -> LogisticRegression\n",
            "  LogisticRegression metrics: f1=0.8735 prec=0.8567 rec=0.8910 acc=0.8710\n",
            "Training (subset) -> LinearSVC\n",
            "  LinearSVC metrics: f1=0.8742 prec=0.8627 rec=0.8860 acc=0.8725\n",
            "Training (subset) -> RandomForest\n",
            "  RandomForest metrics: f1=0.8537 prec=0.8646 rec=0.8430 acc=0.8555\n",
            "Training (subset) -> GradientBoosting\n",
            "  GradientBoosting metrics: f1=0.8410 prec=0.8004 rec=0.8860 acc=0.8325\n",
            "Training (subset) -> MultinomialNB\n",
            "  MultinomialNB metrics: f1=0.8543 prec=0.8648 rec=0.8440 acc=0.8560\n",
            "\n",
            "Best on subset: LinearSVC with F1 = 0.874198322644302\n",
            "\n",
            "Retraining best model on full training set: LinearSVC\n",
            "\n",
            "Final test metrics for LinearSVC : {'f1': 0.910038068523342, 'precision': 0.911682055399438, 'recall': 0.9084, 'accuracy': 0.9102}\n",
            "----- Example 1 -----\n",
            "Review (first 400 chars): 'The Adventures Of Barry McKenzie' started life as a satirical comic strip in 'Private Eye', written by Barry Humphries and based on an idea by Peter Cook. McKenzie ( 'Bazza' to his friends ) is a lanky, loud, hat-wearing Australian whose two main interests in life are sex ( despite never having had any ) and Fosters lager. In 1972, he found his way to the big screen for the first of two outings. \n",
            "True: positive  Pred: positive  Prob_pos: 0.722\n",
            "\n",
            "----- Example 2 -----\n",
            "Review (first 400 chars): For a while it seemed like this show was on 24/7. Then apparently there was a second season or some other kind of continuation of this horrible show about the two most vapid and conceited people who have ever been filmed. All the other comments have captured the essence of these two selfish, haggish, airheads perfectly. Not much less can be said about them besides what everyone else has said.<br /\n",
            "True: negative  Pred: negative  Prob_pos: 0.346\n",
            "\n",
            "----- Example 3 -----\n",
            "Review (first 400 chars): Well it's been a long year and I'm down to reviewing the final film for 2004. Panaghoy Sa Suba (Call of The River) placed second in the recent Metro Manila Film Festival. As expected, it didn't do so well at the box office as it was too artsy for the common moviegoers especially since MMFF is the season where a lot of families go out to see movies.<br /><br />It was quite intriguing to see a movie\n",
            "True: positive  Pred: positive  Prob_pos: 0.678\n",
            "\n",
            "----- Example 4 -----\n",
            "Review (first 400 chars): Call me adolescent but I really do think that this is a great series. If you haven't had a chance to experience a few episodes of the latest Star Trek series, you should definitely watch this one. Perhaps more compelling than that of Voyager's Caretaker, which launched the series with Cpt. Janeway, Archer's adventures are completely different, yet strangely familiar...The music is catchy too. No t\n",
            "True: positive  Pred: positive  Prob_pos: 0.864\n",
            "\n",
            "----- Example 5 -----\n",
            "Review (first 400 chars): A scientist (John Carradine--sadly) finds out how to bring the dead back to life. However they come back with faces of marble. Eventually this all leads to disaster.<br /><br />Boring, totally predictable 1940s outing. This scared me silly when I was a kid but just bores me now. I had to struggle to stay awake! With one exception, the acting is horrible. Such expressionless boring actors! Hopeless\n",
            "True: negative  Pred: negative  Prob_pos: 0.070\n",
            "\n",
            "----- Example 6 -----\n",
            "Review (first 400 chars): I'm going to go on the record as the second person who has, after years of using the IMDb to look up movies, been motivated by Nacho's film, The Abandoned to create an account and post a comment. This was hands down the worst movie I've ever seen in my entire life. The plot was on the verge of non-existence, and none of the \"puzzle-pieces\" added up in any way whatsoever. The acting was laughable a\n",
            "True: negative  Pred: negative  Prob_pos: 0.209\n",
            "\n",
            "----- Example 7 -----\n",
            "Review (first 400 chars): In my eyes this is almost the perfect example of Hollywood ego, only beaten by the new king kong movie. Superman is the original super hero and deserves to be treated with respect even though he wears tights. Brandon Routh was the worst superman I've ever seen, from the start of the movie u just wanna shove a chunk of kryptonite down his throat. He looks just silly wearing the costume. But enough \n",
            "True: negative  Pred: negative  Prob_pos: 0.082\n",
            "\n",
            "----- Example 8 -----\n",
            "Review (first 400 chars): This movie is apparently intended for a young, evangelical Christian audience as a teaching tool. For that I give it a 7 out of 10 point vote. It's a decent movie to show a youth group, but I don't think it will be very well received beyond that. For any other audience, I'd rate it lots lower.<br /><br />The reviewers that saw \"It's a Wonderful Life\" in this were right on, though I didn't think of\n",
            "True: positive  Pred: negative  Prob_pos: 0.399\n",
            "\n",
            "----- Example 9 -----\n",
            "Review (first 400 chars): John Thaw, of Inspector Morse fame, plays old Tom Oakley in this movie. Tom lives in a tiny English village during 1939 and the start of the Second World War. A bit of a recluse, Tom has not yet recovered from the death of his wife and son while he was serving during the First World War. If you can imagine Inspector Morse old and retired, twice as crochety as when he was a policeman, then you've g\n",
            "True: positive  Pred: positive  Prob_pos: 0.664\n",
            "\n",
            "----- Example 10 -----\n",
            "Review (first 400 chars): I rented this one on DVD without any prior knowledge. I was suspicious seeing Michael Madsen appearing in a movie I have never heard of, but it was a freebie, so why not check it out.<br /><br />Well my guess is that Mr. Blonde would very much like to forget he's ever taken part in such a shame of a film.<br /><br />Apparently, if your script and dialogs are terrible, even good actors cannot save \n",
            "True: negative  Pred: negative  Prob_pos: 0.092\n",
            "\n",
            "Saved best_model.pkl and tfidf_vectorizer.pkl\n"
          ]
        }
      ]
    }
  ]
}