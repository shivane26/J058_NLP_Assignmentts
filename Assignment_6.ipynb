{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef7e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import random\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "DEFAULT_EXTRACT_DIR = \"data/quora_question_pairs\"\n",
    "DEFAULT_EXTRACTED_CSV = os.path.join(DEFAULT_EXTRACT_DIR, \"train.csv\")\n",
    "LOCAL_CSV_CANDIDATES = [\n",
    "    \"data/quora/train.csv\",\n",
    "    \"data/train.csv\",\n",
    "    \"dataset/train.csv\",\n",
    "    \"train.csv\",\n",
    "]\n",
    "LOCAL_ZIP_CANDIDATES = [\n",
    "    \"data/quora/train.csv.zip\",\n",
    "    \"data/train.csv.zip\",\n",
    "    \"dataset/train.csv.zip\",\n",
    "    \"train.csv.zip\",\n",
    "]\n",
    "\n",
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def trainer_amp_args() -> Dict[str, bool]:\n",
    "    if torch.cuda.is_available():\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            return {\"bf16\": True, \"fp16\": False}\n",
    "        return {\"bf16\": False, \"fp16\": True}\n",
    "    return {\"bf16\": False, \"fp16\": False}\n",
    "\n",
    "def set_global_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_data_csv(data_csv: Optional[str]) -> str:\n",
    "    if data_csv:\n",
    "        if os.path.exists(data_csv):\n",
    "            print(f\"[data] Using provided CSV: {data_csv}\")\n",
    "            return data_csv\n",
    "        raise FileNotFoundError(f\"--data_csv provided but not found: {data_csv}\")\n",
    "    for cand in LOCAL_CSV_CANDIDATES:\n",
    "        if os.path.exists(cand):\n",
    "            print(f\"[data] Found CSV at: {cand}\")\n",
    "            return cand\n",
    "    for zpath in LOCAL_ZIP_CANDIDATES:\n",
    "        if os.path.exists(zpath):\n",
    "            print(f\"[data] Found ZIP at: {zpath}\")\n",
    "            os.makedirs(DEFAULT_EXTRACT_DIR, exist_ok=True)\n",
    "            with zipfile.ZipFile(zpath, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(DEFAULT_EXTRACT_DIR)\n",
    "            if os.path.exists(DEFAULT_EXTRACTED_CSV):\n",
    "                print(f\"[data] Extracted to: {DEFAULT_EXTRACTED_CSV}\")\n",
    "                return DEFAULT_EXTRACTED_CSV\n",
    "            for root, _, files in os.walk(DEFAULT_EXTRACT_DIR):\n",
    "                if \"train.csv\" in files:\n",
    "                    found = os.path.join(root, \"train.csv\")\n",
    "                    print(f\"[data] Extracted and found: {found}\")\n",
    "                    return found\n",
    "            raise FileNotFoundError(f\"ZIP extracted, but train.csv not found under {DEFAULT_EXTRACT_DIR}\")\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate dataset CSV. Place train.csv in data/quora/ or data/ \"\n",
    "        \"or pass --data_csv /path/to/train.csv. A train.csv.zip in those locations is also supported.\"\n",
    "    )\n",
    "\n",
    "def load_quora_df(csv_path: str, limit_rows: Optional[int] = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Expected columns: id,qid1,qid2,question1,question2,is_duplicate\n",
    "    df = df.dropna(subset=[\"question1\", \"question2\", \"is_duplicate\"]).copy()\n",
    "    df = df.rename(columns={\"is_duplicate\": \"label\"})\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    if limit_rows is not None and limit_rows > 0:\n",
    "        df = df.iloc[:limit_rows].reset_index(drop=True)\n",
    "    return df[[\"question1\", \"question2\", \"label\"]].reset_index(drop=True)\n",
    "\n",
    "def get_or_create_splits(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: str,\n",
    "    seed: int = 42,\n",
    "    test_size: float = 0.1,\n",
    "    val_size_of_train: float = 0.1111,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    splits_dir = os.path.join(output_dir, \"splits\")\n",
    "    train_p = os.path.join(splits_dir, \"train.csv\")\n",
    "    val_p = os.path.join(splits_dir, \"val.csv\")\n",
    "    test_p = os.path.join(splits_dir, \"test.csv\")\n",
    "\n",
    "    if os.path.exists(train_p) and os.path.exists(val_p) and os.path.exists(test_p):\n",
    "        print(f\"[splits] Loading existing splits from {splits_dir}\")\n",
    "        train_df = pd.read_csv(train_p)\n",
    "        val_df = pd.read_csv(val_p)\n",
    "        test_df = pd.read_csv(test_p)\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    print(f\"[splits] Creating new splits and saving to {splits_dir}\")\n",
    "    os.makedirs(splits_dir, exist_ok=True)\n",
    "    train_temp, test_df = model_selection.train_test_split(\n",
    "        df, test_size=test_size, random_state=seed, stratify=df[\"label\"]\n",
    "    )\n",
    "    train_df, val_df = model_selection.train_test_split(\n",
    "        train_temp, test_size=val_size_of_train, random_state=seed, stratify=train_temp[\"label\"]\n",
    "    )\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    train_df.to_csv(train_p, index=False)\n",
    "    val_df.to_csv(val_p, index=False)\n",
    "    test_df.to_csv(test_p, index=False)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def add_sentence_cols(df: pd.DataFrame, label_float: bool = True) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"sentence1\"] = out[\"question1\"]\n",
    "    out[\"sentence2\"] = out[\"question2\"]\n",
    "    out[\"label\"] = out[\"label\"].astype(float if label_float else int)\n",
    "    return out[[\"sentence1\", \"sentence2\", \"label\"]]\n",
    "\n",
    "def make_contrastive_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # ContrastiveLoss expects two separate sentence inputs\n",
    "    return add_sentence_cols(df, label_float=True)\n",
    "\n",
    "def df_to_st_dataset(df: pd.DataFrame) -> Dataset:\n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))\n",
    "\n",
    "def tune_threshold_from_scores(y_true: np.ndarray, scores: np.ndarray) -> float:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "    f1s = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1s[:-1])) if len(thresholds) > 0 else 0\n",
    "    thr = thresholds[best_idx] if len(thresholds) > 0 else 0.5\n",
    "    return float(thr)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_biencoder_f1(\n",
    "    model: SentenceTransformer,\n",
    "    df: pd.DataFrame,\n",
    "    threshold: Optional[float] = None,\n",
    "    batch_size: int = 512,\n",
    ") -> Tuple[float, float]:\n",
    "    device = model.device if hasattr(model, \"device\") else get_device()\n",
    "    sents1 = df[\"question1\"].tolist()\n",
    "    sents2 = df[\"question2\"].tolist()\n",
    "    emb1 = model.encode(sents1, batch_size=batch_size, convert_to_tensor=True,\n",
    "                        normalize_embeddings=True, show_progress_bar=False, device=device)\n",
    "    emb2 = model.encode(sents2, batch_size=batch_size, convert_to_tensor=True,\n",
    "                        normalize_embeddings=True, show_progress_bar=False, device=device)\n",
    "    sims = (emb1 * emb2).sum(dim=1).detach().cpu().numpy()\n",
    "    y_true = df[\"label\"].to_numpy().astype(int)\n",
    "    thr = threshold if threshold is not None else tune_threshold_from_scores(y_true, sims)\n",
    "    y_pred = (sims >= thr).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return float(f1), float(thr)\n",
    "\n",
    "def evaluate_crossencoder_f1(\n",
    "    model: CrossEncoder,\n",
    "    df: pd.DataFrame,\n",
    "    threshold: Optional[float] = None,\n",
    "    batch_size: int = 256,\n",
    ") -> Tuple[float, float]:\n",
    "    pairs = list(zip(df[\"question1\"].tolist(), df[\"question2\"].tolist()))\n",
    "    scores = np.array(model.predict(pairs, batch_size=batch_size, show_progress_bar=False)).reshape(-1)\n",
    "    y_true = df[\"label\"].to_numpy().astype(int)\n",
    "    thr = threshold if threshold is not None else tune_threshold_from_scores(y_true, scores)\n",
    "    y_pred = (scores >= thr).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return float(f1), float(thr)\n",
    "\n",
    "def safe_write_json(path: str, data: Dict):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25774c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Using provided CSV: data/quora_question_pairs/train.csv\n",
      "[splits] Loading existing splits from outputs_quora/splits\n",
      "(323433, 3) (40425, 3) (40429, 3)\n"
     ]
    }
   ],
   "source": [
    "## Cell 3 â€” Config (edit paths/models here)\n",
    "# Path to your Kaggle CSV; leave empty to auto-detect known locations\n",
    "DATA_CSV = \"data/quora_question_pairs/train.csv\"  # set to \"\" to auto-locate\n",
    "OUTPUT_DIR = \"outputs_quora\"\n",
    "\n",
    "# Models\n",
    "BI_MODEL = \"microsoft/xtremedistil-l6-h256-uncased\"\n",
    "CE_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "# Training/eval\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_BS = 64\n",
    "EVAL_BS = 256\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_SEQ_LENGTH = 256\n",
    "SEED = 42\n",
    "LIMIT_ROWS = 0  # e.g., 20000 for a quick dry-run\n",
    "\n",
    "# Ensure dirs exist and seed set\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "# Load data / splits (idempotent)\n",
    "csv_path = ensure_data_csv(DATA_CSV if DATA_CSV else None)\n",
    "df_all = load_quora_df(csv_path, limit_rows=LIMIT_ROWS if LIMIT_ROWS > 0 else None)\n",
    "train_df, val_df, test_df = get_or_create_splits(df_all, OUTPUT_DIR, seed=SEED)\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2950d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/xtremedistil-l6-h256-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "## Cell 4 â€” Experiment 1: Baseline bi-encoder (no fine-tuning)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = get_device()\n",
    "model = SentenceTransformer(BI_MODEL, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "f1_val, thr_val = evaluate_biencoder_f1(model, val_df, threshold=None, batch_size=EVAL_BS)\n",
    "f1_test, thr_test = evaluate_biencoder_f1(model, test_df, threshold=thr_val, batch_size=EVAL_BS)\n",
    "\n",
    "res = {\n",
    "    \"experiment\": \"baseline_biencoder\",\n",
    "    \"val_f1\": f1_val, \"val_threshold\": thr_val,\n",
    "    \"test_f1\": f1_test, \"test_threshold\": thr_test,\n",
    "    \"model_name\": BI_MODEL\n",
    "}\n",
    "save_path = os.path.join(OUTPUT_DIR, \"baseline_biencoder\", \"result.json\")\n",
    "safe_write_json(save_path, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0aad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/xtremedistil-l6-h256-uncased. Creating a new one with mean pooling.\n",
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5054' max='5054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5054/5054 04:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dev-cosine Pearson Cosine</th>\n",
       "      <th>Dev-cosine Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.152555</td>\n",
       "      <td>0.639202</td>\n",
       "      <td>0.645726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Cell 5 â€” Experiment 2: Bi-encoder (CosineSimilarityLoss)\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "device = get_device()\n",
    "model = SentenceTransformer(BI_MODEL, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "train_ds = df_to_st_dataset(add_sentence_cols(train_df, label_float=True))\n",
    "val_ds = df_to_st_dataset(add_sentence_cols(val_df, label_float=True))\n",
    "\n",
    "loss = losses.CosineSimilarityLoss(model=model)\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=val_df[\"question1\"].tolist(),\n",
    "    sentences2=val_df[\"question2\"].tolist(),\n",
    "    scores=val_df[\"label\"].astype(float).tolist(),\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"dev-cosine\",\n",
    ")\n",
    "\n",
    "mp = trainer_amp_args()\n",
    "out_dir = os.path.join(OUTPUT_DIR, \"bi_cosine\")\n",
    "args_tr = SentenceTransformerTrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=mp[\"fp16\"], bf16=mp[\"bf16\"],\n",
    "    eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=2,\n",
    "    report_to=\"none\", logging_steps=100, load_best_model_at_end=False,\n",
    "    remove_unused_columns=False, dataloader_num_workers=min(4, os.cpu_count() or 1),\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model, args=args_tr, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    loss=loss, evaluator=evaluator\n",
    ")\n",
    "trainer.train()\n",
    "model.save(out_dir)\n",
    "\n",
    "# Reload and evaluate\n",
    "model = SentenceTransformer(out_dir, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "f1_val, thr_val = evaluate_biencoder_f1(model, val_df, threshold=None, batch_size=EVAL_BS)\n",
    "f1_test, thr_test = evaluate_biencoder_f1(model, test_df, threshold=thr_val, batch_size=EVAL_BS)\n",
    "\n",
    "res = {\n",
    "    \"experiment\": \"bi_cosine\",\n",
    "    \"val_f1\": f1_val, \"val_threshold\": thr_val,\n",
    "    \"test_f1\": f1_test, \"test_threshold\": thr_test,\n",
    "    \"save_dir\": out_dir\n",
    "}\n",
    "safe_write_json(os.path.join(out_dir, \"result.json\"), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d1dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/xtremedistil-l6-h256-uncased. Creating a new one with mean pooling.\n",
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5054' max='5054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5054/5054 04:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dev-contrastive Pearson Cosine</th>\n",
       "      <th>Dev-contrastive Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.602302</td>\n",
       "      <td>0.663451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Cell 6 â€” Experiment 3: Bi-encoder (ContrastiveLoss)\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "device = get_device()\n",
    "model = SentenceTransformer(BI_MODEL, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "# IMPORTANT: two sentence columns for contrastive\n",
    "train_ds = df_to_st_dataset(make_contrastive_df(train_df))\n",
    "val_ds = df_to_st_dataset(make_contrastive_df(val_df))\n",
    "\n",
    "loss = losses.ContrastiveLoss(model=model)\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=val_df[\"question1\"].tolist(),\n",
    "    sentences2=val_df[\"question2\"].tolist(),\n",
    "    scores=val_df[\"label\"].astype(float).tolist(),\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"dev-contrastive\",\n",
    ")\n",
    "\n",
    "mp = trainer_amp_args()\n",
    "out_dir = os.path.join(OUTPUT_DIR, \"bi_contrastive\")\n",
    "args_tr = SentenceTransformerTrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=mp[\"fp16\"], bf16=mp[\"bf16\"],\n",
    "    eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=2,\n",
    "    report_to=\"none\", logging_steps=100, load_best_model_at_end=False,\n",
    "    remove_unused_columns=False, dataloader_num_workers=min(4, os.cpu_count() or 1),\n",
    ")\n",
    "\n",
    "# Sanity check to avoid the earlier assertion\n",
    "assert set([\"sentence1\", \"sentence2\", \"label\"]).issubset(set(train_ds.column_names)), train_ds.column_names\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model, args=args_tr, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    loss=loss, evaluator=evaluator\n",
    ")\n",
    "trainer.train()\n",
    "model.save(out_dir)\n",
    "\n",
    "model = SentenceTransformer(out_dir, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "f1_val, thr_val = evaluate_biencoder_f1(model, val_df, threshold=None, batch_size=EVAL_BS)\n",
    "f1_test, thr_test = evaluate_biencoder_f1(model, test_df, threshold=thr_val, batch_size=EVAL_BS)\n",
    "\n",
    "res = {\n",
    "    \"experiment\": \"bi_contrastive\",\n",
    "    \"val_f1\": f1_val, \"val_threshold\": thr_val,\n",
    "    \"test_f1\": f1_test, \"test_threshold\": thr_test,\n",
    "    \"save_dir\": out_dir\n",
    "}\n",
    "safe_write_json(os.path.join(out_dir, \"result.json\"), res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0599f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/xtremedistil-l6-h256-uncased. Creating a new one with mean pooling.\n",
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1866' max='1866' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1866/1866 02:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dev-mnrl Pearson Cosine</th>\n",
       "      <th>Dev-mnrl Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>1.545256</td>\n",
       "      <td>0.471690</td>\n",
       "      <td>0.530116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Cell 7 â€” Experiment 4: Bi-encoder (MultipleNegativesRankingLoss)\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "device = get_device()\n",
    "model = SentenceTransformer(BI_MODEL, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "train_pos = train_df[train_df[\"label\"] == 1].copy()\n",
    "train_ds = df_to_st_dataset(add_sentence_cols(train_pos, label_float=True))\n",
    "val_ds = df_to_st_dataset(add_sentence_cols(val_df, label_float=True))\n",
    "\n",
    "loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=val_df[\"question1\"].tolist(),\n",
    "    sentences2=val_df[\"question2\"].tolist(),\n",
    "    scores=val_df[\"label\"].astype(float).tolist(),\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"dev-mnrl\",\n",
    ")\n",
    "\n",
    "mp = trainer_amp_args()\n",
    "out_dir = os.path.join(OUTPUT_DIR, \"bi_mnrl\")\n",
    "args_tr = SentenceTransformerTrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=mp[\"fp16\"], bf16=mp[\"bf16\"],\n",
    "    eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=2,\n",
    "    report_to=\"none\", logging_steps=100, load_best_model_at_end=False,\n",
    "    remove_unused_columns=False, dataloader_num_workers=min(4, os.cpu_count() or 1),\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model, args=args_tr, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    loss=loss, evaluator=evaluator\n",
    ")\n",
    "trainer.train()\n",
    "model.save(out_dir)\n",
    "\n",
    "model = SentenceTransformer(out_dir, device=device)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "f1_val, thr_val = evaluate_biencoder_f1(model, val_df, threshold=None, batch_size=EVAL_BS)\n",
    "f1_test, thr_test = evaluate_biencoder_f1(model, test_df, threshold=thr_val, batch_size=EVAL_BS)\n",
    "\n",
    "res = {\n",
    "    \"experiment\": \"bi_mnrl\",\n",
    "    \"val_f1\": f1_val, \"val_threshold\": thr_val,\n",
    "    \"test_f1\": f1_test, \"test_threshold\": thr_test,\n",
    "    \"save_dir\": out_dir\n",
    "}\n",
    "safe_write_json(os.path.join(out_dir, \"result.json\"), res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e9db91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10108' max='10108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10108/10108 06:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val-ce Accuracy</th>\n",
       "      <th>Val-ce Accuracy Threshold</th>\n",
       "      <th>Val-ce F1</th>\n",
       "      <th>Val-ce F1 Threshold</th>\n",
       "      <th>Val-ce Precision</th>\n",
       "      <th>Val-ce Recall</th>\n",
       "      <th>Val-ce Average Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862956</td>\n",
       "      <td>0.263354</td>\n",
       "      <td>0.821417</td>\n",
       "      <td>-0.314074</td>\n",
       "      <td>0.767904</td>\n",
       "      <td>0.882948</td>\n",
       "      <td>0.879187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4042</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>-0.104631</td>\n",
       "      <td>0.839009</td>\n",
       "      <td>-0.434809</td>\n",
       "      <td>0.792389</td>\n",
       "      <td>0.891457</td>\n",
       "      <td>0.898017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6063</td>\n",
       "      <td>0.293100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883686</td>\n",
       "      <td>0.142398</td>\n",
       "      <td>0.849901</td>\n",
       "      <td>-0.127329</td>\n",
       "      <td>0.811658</td>\n",
       "      <td>0.891926</td>\n",
       "      <td>0.908089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8084</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.891404</td>\n",
       "      <td>-0.117452</td>\n",
       "      <td>0.858282</td>\n",
       "      <td>-0.360567</td>\n",
       "      <td>0.825363</td>\n",
       "      <td>0.893936</td>\n",
       "      <td>0.917659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10105</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>-0.269954</td>\n",
       "      <td>0.857097</td>\n",
       "      <td>-0.496820</td>\n",
       "      <td>0.816802</td>\n",
       "      <td>0.901575</td>\n",
       "      <td>0.917374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'experiment': 'cross_encoder',\n",
       " 'val_f1': 0.8582824059183017,\n",
       " 'val_threshold': -0.36038997769355774,\n",
       " 'test_f1': 0.856766905228024,\n",
       " 'test_threshold': -0.36038997769355774,\n",
       " 'save_dir': 'outputs_quora/cross_encoder'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8 â€” Experiment 5: Cross-encoder (fixed evaluator)\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "\n",
    "# IMPORTANT: use the CrossEncoder evaluator, not the bi-encoder one\n",
    "try:\n",
    "    from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "    has_ce_eval = True\n",
    "except Exception:\n",
    "    has_ce_eval = False\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "train_samples = [InputExample(texts=[r.question1, r.question2], label=float(r.label))\n",
    "                 for r in train_df.itertuples(index=False)]\n",
    "val_samples = [InputExample(texts=[r.question1, r.question2], label=float(r.label))\n",
    "               for r in val_df.itertuples(index=False)]\n",
    "\n",
    "ce_dir = os.path.join(OUTPUT_DIR, \"cross_encoder\")\n",
    "os.makedirs(ce_dir, exist_ok=True)\n",
    "\n",
    "ce = CrossEncoder(CE_MODEL, num_labels=1, device=device, max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "# Build the proper evaluator for CrossEncoder\n",
    "if has_ce_eval:\n",
    "    # Preferred path (sentence-transformers >= 2.x)\n",
    "    try:\n",
    "        evaluator = CEBinaryClassificationEvaluator.from_input_examples(val_samples, name=\"val-ce\")\n",
    "    except TypeError:\n",
    "        # Fallback signature for older versions\n",
    "        s1 = [t.texts[0] for t in val_samples]\n",
    "        s2 = [t.texts[1] for t in val_samples]\n",
    "        y = [float(t.label) for t in val_samples]\n",
    "        evaluator = CEBinaryClassificationEvaluator(s1, s2, y, name=\"val-ce\")\n",
    "else:\n",
    "    evaluator = None  # if import fails, skip in-training eval (we still evaluate after training)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_samples,\n",
    "    shuffle=True,\n",
    "    batch_size=max(8, min(64, TRAIN_BS // 2)),\n",
    "    num_workers=min(4, os.cpu_count() or 1),\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * int(EPOCHS)\n",
    "warmup_steps = int(total_steps * float(WARMUP_RATIO))\n",
    "use_amp = (device == \"cuda\")\n",
    "\n",
    "ce.fit(\n",
    "    train_dataloader=train_loader,\n",
    "    evaluator=evaluator,  # can be None\n",
    "    epochs=int(EPOCHS),\n",
    "    warmup_steps=warmup_steps,           # CrossEncoder.fit expects steps, not ratio\n",
    "    optimizer_params={\"lr\": float(LEARNING_RATE)},\n",
    "    show_progress_bar=True,\n",
    "    evaluation_steps=max(500, len(train_loader) // 5) if evaluator is not None else 0,\n",
    "    output_path=ce_dir,\n",
    "    use_amp=use_amp,\n",
    ")\n",
    "\n",
    "# Reload and evaluate\n",
    "ce = CrossEncoder(ce_dir, num_labels=1, device=device, max_length=MAX_SEQ_LENGTH)\n",
    "f1_val, thr_val = evaluate_crossencoder_f1(ce, val_df, threshold=None, batch_size=min(256, EVAL_BS))\n",
    "f1_test, thr_test = evaluate_crossencoder_f1(ce, test_df, threshold=thr_val, batch_size=min(256, EVAL_BS))\n",
    "\n",
    "res = {\n",
    "    \"experiment\": \"cross_encoder\",\n",
    "    \"val_f1\": f1_val, \"val_threshold\": thr_val,\n",
    "    \"test_f1\": f1_test, \"test_threshold\": thr_test,\n",
    "    \"save_dir\": ce_dir\n",
    "}\n",
    "safe_write_json(os.path.join(ce_dir, \"result.json\"), res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f9f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_encoder     Test F1: 0.8568  (Val F1: 0.8583, thr_val=-0.3604)\n",
      "bi_contrastive    Test F1: 0.7690  (Val F1: 0.7703, thr_val=0.7891)\n",
      "bi_cosine         Test F1: 0.7552  (Val F1: 0.7573, thr_val=0.5904)\n",
      "bi_mnrl           Test F1: 0.6975  (Val F1: 0.6990, thr_val=0.7489)\n",
      "baseline_biencoder  Test F1: 0.6043  (Val F1: 0.6041, thr_val=0.9696)\n"
     ]
    }
   ],
   "source": [
    "## Cell 9 â€” Load summary across completed experiments\n",
    "import glob, json, os\n",
    "\n",
    "results = []\n",
    "for path in glob.glob(os.path.join(OUTPUT_DIR, \"*\", \"result.json\")):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            results.append(json.load(f))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read\", path, e)\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[\"test_f1\"], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['experiment']:16s}  Test F1: {r['test_f1']:.4f}  (Val F1: {r['val_f1']:.4f}, thr_val={r['val_threshold']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c825dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
